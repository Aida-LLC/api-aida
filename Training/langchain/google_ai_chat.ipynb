{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/langchain-google-genai/\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.9.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain-google-genai pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAIes7F5Af0XvA5IVV3umNKMpCXzvpMLdY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "result = llm.invoke(\"Write a ballad about LangChain\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"style\"],\n",
    "    template=\"Please answer the following question: {question} in the style of {style}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "genAi = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo, fam! We feel anger and love cuz they're like, totally human emotions, ya feel? Anger is like, when you're all fired up and ready to throw a fit, right? It's like, when your bestie cancels on you last minute, you're gonna be mad AF. But love is like, when your heart's all mushy and you wanna cuddle with your boo. It's like, when you see your crush and your tummy does a lil' flip. So, yeah, anger and love are just part of the human experience, and it's all good, bae.\n"
     ]
    }
   ],
   "source": [
    "response = genAi.run(question=\"Why do we feel anger and love?\", style=\"GenZ teen\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo, fam! We feel anger and love cuz they're like, totally human emotions, ya\n",
      "feel? Anger is like, when you're all fired up and ready to throw a fit, right?\n",
      "It's like, when your bestie cancels on you last minute, you're gonna be mad AF.\n",
      "But love is like, when your heart's all mushy and you wanna cuddle with your\n",
      "boo. It's like, when you see your crush and your tummy does a lil' flip. So,\n",
      "yeah, anger and love are just part of the human experience, and it's all good,\n",
      "bae.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "def wrap_text(text, width=80):\n",
    "    return textwrap.fill(text, width)\n",
    "\n",
    "print(wrap_text(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pypdf\n",
    "# !pip install langchain_community --upgrade\n",
    "# !pip install faiss-cpu --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../data/software_inspection.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index = FAISS.from_documents(pages, GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 9:\n",
      "6.6.2 Inspectors and Roles\n",
      "There are four inspector roles identiﬁed in a Fagan Inspection and they are\n",
      "described in Table 6.7.\n",
      "6.6.3 Inspection Entry Criteria\n",
      "There are explicit entry and exit criteria deﬁned for the various types of inspections.\n",
      "These criteria need to be satisﬁed to ensure that the inspection is effective. The\n",
      "entry criteria for the various inspections are given in Table 6.8.\n",
      "6.6.4 Preparation\n",
      "Preparation is a key part of the inspection process, as the inspection will be\n",
      "ineffe\n",
      "\n",
      "\n",
      "\n",
      "Page 7:\n",
      "6.6 Fagan Inspections\n",
      "The Fagan methodology (Table 6.4) is a well-known software inspection\n",
      "methodology. It is a seven-step process and includes planning, overview, prepara-\n",
      "tion, inspection meeting, process improvement, re-work, and follow-up activity.\n",
      "Its objectives are to identify and remove errors in the work products, and also to\n",
      "identify any systemic defects in the processes used to create the work products.\n",
      "The Fagan inspection process stipulates that requirement documents, design\n",
      "documen\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is fagan inspection?\"\n",
    "\n",
    "\n",
    "docs = faiss_index.similarity_search(query, k=2)\n",
    "for doc in docs:\n",
    "    doc_page = doc.metadata[\"page\"]\n",
    "    print(f\"Page {doc_page}:\")\n",
    "\n",
    "    print(doc.page_content[:500])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 9: 0.5453141927719116 \n",
      " 6.6.2 Inspectors and Roles\n",
      "There are four inspector roles identiﬁed in a Fagan Inspection and they a \n",
      "\n",
      "\n",
      "Page 7: 0.5675018429756165 \n",
      " 6.6 Fagan Inspections\n",
      "The Fagan methodology (Table 6.4) is a well-known software inspection\n",
      "methodol \n",
      "\n",
      "\n",
      "Page 17: 0.5806626081466675 \n",
      " informal and usually takes place at the author’s desk or in a meeting room, and the\n",
      "reviewer and aut \n",
      "\n",
      "\n",
      "Page 0: 0.5858128070831299 \n",
      " Software Inspections6\n",
      "Key Topics\n",
      "Fagan Inspection\n",
      "Gilb Inspections\n",
      "Economic Beneﬁts of Inspection\n",
      "In \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_with_scores = faiss_index.similarity_search_with_score(query)\n",
    "\n",
    "for item in search_with_scores:\n",
    "    print(f\"Page {item[0].metadata['page']}: {item[1]} \\n {item[0].page_content[:100]} \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb\n",
    "# !pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", convert_system_message_to_human=True)\n",
    "chain = load_qa_chain(llm=qa_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This blog post discusses the concept of building autonomous agents with LLMs (large language models) as their central controller, providing an overview of the agent system and its key components: planning, memory, and tool use. The author highlights proof-of-concept demonstrations such as AutoGPT, GPT-Engineer, and BabyAGI, and explores different approaches to task decomposition, self-reflection, and external tool integration. The post also discusses challenges faced by LLM-powered agents, including finite context length, limitations in long-term planning, and the reliability of natural language interfaces.'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Write a short summary of the blog post.\"\n",
    "response = chain.invoke({\"input_documents\": docs, \"question\": query})\n",
    "response['output_text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
